{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa308c39-2249-4fce-aee8-bcc4a9ca705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a4089-dd32-4935-8819-d882115a544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Both RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are measures of the difference between the predicted and actual values of the dependent variable in regression analysis.\n",
    "\n",
    "If we value the impact of large errors more than smaller errors, we might prefer to use RMSE, as it puts more weight on larger errors than MAE.\n",
    "On the other hand, if we value all errors equally and want to minimize the average size of errors, we might prefer to use MAE.\n",
    "In the given scenario, since RMSE is higher for Model A compared to the MAE of Model B, we can say that Model B has better performance, as it has a lower average error. \n",
    "However, it is important to consider the limitations of the metric being used.\n",
    "For example, both RMSE and MAE are sensitive to outliers in the data, and a single outlier can significantly impact the metric value. \n",
    "\n",
    "Therefore, it is important to investigate the nature of the errors and assess whether the chosen metric is appropriate for the specific problem being addressed.\n",
    "Additionally, it is always recommended to compare the performance of models using multiple evaluation metrics to obtain a more comprehensive understanding of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc782b-3340-4629-b804-4479fde63b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8874e6e-849b-434f-8836-1fbd2f2cf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of which regularized linear model is better depends on the specific needs and requirements of the problem being addressed.\n",
    "Both Ridge regularization and Lasso regularization are types of regularization used to prevent overfitting in linear regression models.\n",
    "\n",
    "Ridge regression adds a penalty term to the linear regression cost function that is proportional to the square of the magnitude of the coefficients.\n",
    "This penalty term shrinks the coefficients towards zero and can help reduce overfitting by reducing the variance in the model.\n",
    "\n",
    "Lasso regression, on the other hand, adds a penalty term that is proportional to the absolute value of the coefficients. \n",
    "This type of regularization can lead to sparse models where some of the coefficients are exactly zero, effectively performing feature selection and reducing the complexity of the model.\n",
    "\n",
    "In the given scenario, since the regularization parameters and types are different for both models, it is difficult to directly compare their performance.\n",
    "However, we can say that Model A may be better at reducing the variance in the model due to its lower regularization parameter, while Model B may be better at reducing the complexity of the model and performing feature selection.\n",
    "\n",
    "It is important to consider the trade-offs and limitations of each regularization method. \n",
    "Ridge regression may not perform well when there are many irrelevant features in the data, while Lasso regression can produce unstable and non-unique solutions when the number of features is greater than the number of observations. \n",
    "Therefore, it is important to evaluate the performance of both models using appropriate evaluation metrics and cross-validation techniques, and select the one that best suits the needs of the analysis. \n",
    "Additionally, a combination of both regularization methods, called Elastic Net regularization, can be used to balance the trade-offs of both Ridge and Lasso regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ce283-c9a5-4eb9-8da4-1fdba2c98d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b294b3-7412-4d8f-ba62-a1f766f1ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of RMSE:\n",
    "\n",
    "RMSE is more sensitive to large errors than smaller ones, making it a useful metric when we want to avoid large errors in predictions.\n",
    "RMSE is a widely used metric, making it easy to compare the performance of different models and techniques.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "RMSE can be sensitive to outliers in the data, as it penalizes large errors more heavily than MAE.\n",
    "RMSE is dependent on the scale of the dependent variable, which can make it difficult to compare the performance of models with different units of measurement.\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "MSE is a widely used metric that is relatively easy to interpret and calculate.\n",
    "MSE provides a more detailed understanding of the distribution of errors than RMSE, as it measures the average of the squared errors.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "\n",
    "MSE can be heavily influenced by outliers in the data, as it squares the errors, amplifying the effect of outliers.\n",
    "MSE is dependent on the scale of the dependent variable, which can make it difficult to compare the performance of models with different units of measurement.\n",
    "\n",
    "Advantages of MAE:\n",
    "\n",
    "MAE is less sensitive to outliers in the data than RMSE and MSE, as it calculates the absolute differences between the predicted and actual values.\n",
    "MAE is less dependent on the scale of the dependent variable, making it easier to compare the performance of models with different units of measurement.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "MAE does not distinguish between the direction of errors, meaning that positive and negative errors are treated equally.\n",
    "MAE may not capture the importance of large errors as effectively as RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17647e54-2e0e-46f6-a043-a9e8344cb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be15a58-610b-4b50-9855-0854e24ce948",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a method used to prevent overfitting in linear regression models by adding a penalty term to the cost function that shrinks the coefficient estimates towards zero. \n",
    "Unlike Ridge regularization, Lasso regularization uses the absolute value of the coefficient estimates instead of the squared values.\n",
    "\n",
    "The primary difference between Lasso and Ridge regularization is that Lasso has the ability to set the coefficients of some of the features to zero, effectively performing feature selection.\n",
    "In contrast, Ridge regularization only shrinks the coefficients towards zero but does not set them to zero, meaning that all features are retained.\n",
    "\n",
    "Lasso regularization is more appropriate to use when dealing with high-dimensional datasets where many of the features may not be relevant to the outcome of interest.\n",
    "By setting the coefficients of irrelevant features to zero, Lasso can improve the interpretability and predictive performance of the model.\n",
    "In contrast, Ridge regularization is more appropriate to use when all features are expected to be relevant to the outcome of interest, as it does not perform feature selection.\n",
    "\n",
    "It should be noted that Lasso regularization can be sensitive to the scale of the features, so it is important to standardize the features before applying Lasso regularization. \n",
    "Additionally, Lasso regularization can be computationally expensive, particularly when dealing with a large number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cbdb63-7ddf-4476-a410-0ffb3b39a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf79f2a-5d89-404a-afa5-069c56ce3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the cost function that discourages the model from relying too heavily on any one feature. \n",
    "This penalty term, which is based on the magnitude of the coefficient estimates, can be adjusted to balance the trade-off between fitting the training data and keeping the model simple.\n",
    "\n",
    "For example, let's consider a linear regression problem where we are trying to predict housing prices based on a set of features such as square footage, number of bedrooms, and age of the house. Without regularization, the model may fit the training data very closely by assigning large coefficients to certain features that have little predictive power. However, this can lead to overfitting, where the model performs poorly on new, unseen data.\n",
    "\n",
    "To prevent overfitting, we can apply Ridge or Lasso regularization to the linear regression model.\n",
    "Ridge regularization adds a penalty term to the cost function that is proportional to the sum of the squared coefficient estimates.\n",
    "This encourages the model to shrink the coefficient estimates towards zero, which reduces the model's reliance on any one feature. \n",
    "Similarly, Lasso regularization adds a penalty term to the cost function that is proportional to the sum of the absolute value of the coefficient estimates. \n",
    "In addition to shrinking the coefficients, Lasso also has the ability to set some of the coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "By applying regularization to the linear regression model, we can balance the trade-off between fitting the training data and keeping the model simple, thereby reducing the risk of overfitting. \n",
    "This can result in improved performance on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58577c8d-2976-40bc-95f1-8f67353fd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29734042-a164-4302-b9fd-3d85143734be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Limited flexibility: Regularized linear models are inherently linear and may not be able to capture complex non-linear relationships between the input variables and the target variable. In such cases, other non-linear models such as decision trees, random forests, or neural networks may be more appropriate.\n",
    "\n",
    "Feature selection bias: While Lasso regularization can be used for feature selection by setting some of the coefficients to zero, this can lead to bias if important features are mistakenly excluded from the model. In some cases, a more traditional feature selection approach may be preferred.\n",
    "\n",
    "Sensitivity to hyperparameters: Regularized linear models have hyperparameters that need to be tuned for optimal performance. If the hyperparameters are not chosen carefully, the model may underfit or overfit the data.\n",
    "\n",
    "Limited interpretability: Regularized linear models can be more difficult to interpret than traditional linear regression models, particularly when Lasso regularization is used. The coefficients can be harder to interpret due to the feature selection bias, and it may not always be clear which features are most important for the model's predictions.\n",
    "\n",
    "Limited generalization: Regularized linear models may not always generalize well to new data, particularly if the data distribution changes significantly. In such cases, it may be necessary to retrain the model with new data or consider other approaches.\n",
    "\n",
    "In summary, while regularized linear models are a powerful tool for preventing overfitting in regression analysis, they have certain limitations that should be considered when choosing a modeling approach.\n",
    "It is important to evaluate the specific problem at hand and choose the modeling approach that is best suited to the problem's requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd237345-3e08-4e53-90b9-fd5f616a23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3d772-ef2c-4eaa-8355-94653fff0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared (R²) is a statistical measure that represents the proportion of variance in the dependent variable that is explained by the independent variable(s) in a linear regression model. \n",
    "It is also known as the coefficient of determination.\n",
    "The value of R² ranges from 0 to 1, with higher values indicating a better fit of the model to the data.\n",
    "\n",
    "To calculate R², the total sum of squares (TSS), explained sum of squares (ESS), and residual sum of squares (RSS) are computed. The TSS represents the total variability in the dependent variable and is calculated as the sum of the squared differences between each observation and the mean of the dependent variable. The ESS represents the variability in the dependent variable that is explained by the independent variable(s) and is calculated as the sum of the squared differences between the predicted values and the mean of the dependent variable. The RSS represents the unexplained variability in the dependent variable and is calculated as the sum of the squared differences between the actual values and the predicted values.\n",
    "\n",
    "The formula for R² is:\n",
    "\n",
    "R² = ESS/TSS = 1 - RSS/TSS\n",
    "\n",
    "An R² value of 1 indicates that the model perfectly fits the data, with all of the variability in the dependent variable explained by the independent variable(s).\n",
    "An R² value of 0 indicates that the model does not fit the data at all, with none of the variability in the dependent variable explained by the independent variable(s). \n",
    "In practice, R² values between 0.7 and 0.9 are considered to indicate a strong fit, while values below 0.5 indicate a weak fit.\n",
    "\n",
    "R² is a useful metric for evaluating the performance of linear regression models, as it provides an indication of how well the model fits the data.\n",
    "However, it should be used in conjunction with other evaluation metrics, such as RMSE or MAE, to gain a more complete understanding of the model's performance. \n",
    "Additionally, R² should be interpreted with caution, as a high R² value does not necessarily mean that the model is accurate or useful in making predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb751127-638c-4724-9841-486eff4442bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e1887-ba7a-461a-aaaa-d759d973d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent variables in a linear regression model. It is designed to address a potential limitation of the regular R-squared, which is that it can increase as more independent variables are added to the model, even if those variables do not actually improve the model's predictive power.\n",
    "\n",
    "Adjusted R-squared is calculated using the following formula:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "where:\n",
    "\n",
    "R-squared is the regular R-squared value\n",
    "n is the number of observations in the dataset\n",
    "k is the number of independent variables in the model\n",
    "The adjusted R-squared value ranges from 0 to 1, with higher values indicating a better fit of the model to the data. \n",
    "Like the regular R-squared, an adjusted R-squared value of 1 indicates a perfect fit of the model to the data, while a value of 0 indicates that the model does not fit the data at all.\n",
    "\n",
    "Compared to the regular R-squared, the adjusted R-squared penalizes the addition of independent variables that do not improve the model's performance.\n",
    "This is accomplished by subtracting a penalty term from the regular R-squared based on the number of independent variables in the model. As a result, the adjusted R-squared is often considered to be a more accurate measure of a model's predictive power, especially when comparing models with different numbers of independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05662d9d-7108-440a-8053-41f35044ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1441c-768d-44db-aef8-d12d9829d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "It is generally more appropriate to use adjusted R-squared when comparing the predictive power of linear regression models that have different numbers of independent variables.\n",
    "The regular R-squared tends to increase as more independent variables are added to the model, even if those variables do not actually improve the model's predictive power. \n",
    "This can lead to overestimating the model's performance and falsely concluding that additional independent variables are important.\n",
    "\n",
    "The adjusted R-squared corrects for this potential issue by taking into account the number of independent variables in the model.\n",
    "The penalty term in the adjusted R-squared formula reduces the value of the metric when additional independent variables do not significantly improve the model's performance.\n",
    "This makes it a more appropriate metric for comparing models with different numbers of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e3d15-e35d-455e-aa12-3b1f9a3b1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc9d07-21c1-48f7-8d85-477809f695b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n regression analysis, RMSE, MSE, and MAE are commonly used evaluation metrics to measure the performance of a regression model.\n",
    "\n",
    "RMSE stands for Root Mean Squared Error and represents the square root of the average of the squared differences between the predicted values and the actual values. It is calculated as:\n",
    "\n",
    "RMSE = sqrt((1/n) * sum((y_pred - y_actual)^2))\n",
    "\n",
    "where y_pred is the predicted value, y_actual is the actual value, and n is the number of samples.\n",
    "\n",
    "MSE stands for Mean Squared Error and represents the average of the squared differences between the predicted values and the actual values. It is calculated as:\n",
    "\n",
    "MSE = (1/n) * sum((y_pred - y_actual)^2)\n",
    "\n",
    "where y_pred is the predicted value, y_actual is the actual value, and n is the number of samples.\n",
    "\n",
    "MAE stands for Mean Absolute Error and represents the average of the absolute differences between the predicted values and the actual values. It is calculated as:\n",
    "\n",
    "MAE = (1/n) * sum(abs(y_pred - y_actual))\n",
    "\n",
    "where y_pred is the predicted value, y_actual is the actual value, and n is the number of samples.\n",
    "\n",
    "All three metrics provide a numerical value that represents the performance of the model. The lower the value of the RMSE, MSE, or MAE, the better the model's performance. RMSE and MSE give more weight to large errors, while MAE treats all errors equally. Therefore, the choice of metric depends on the problem at hand and the specific requirements of the application.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
